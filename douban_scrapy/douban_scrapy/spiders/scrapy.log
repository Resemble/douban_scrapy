2016-12-04 11:37:05 [scrapy] INFO: Scrapy 1.1.3 started (bot: douban_scrapy)
2016-12-04 11:37:05 [scrapy] INFO: Overridden settings: {'CONCURRENT_REQUESTS_PER_DOMAIN': 100, 'NEWSPIDER_MODULE': 'douban_scrapy.spiders', 'SPIDER_MODULES': ['douban_scrapy.spiders'], 'CONCURRENT_REQUESTS': 1, 'LOG_FILE': 'scrapy.log', 'BOT_NAME': 'douban_scrapy'}
2016-12-04 11:37:05 [scrapy] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2016-12-04 11:37:05 [scrapy] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'douban_scrapy.middlewares.RandomUserAgent',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2016-12-04 11:37:05 [scrapy] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2016-12-04 11:37:05 [scrapy] INFO: Enabled item pipelines:
['douban_scrapy.pipelines.MySQLStorePipeline']
2016-12-04 11:37:05 [twisted] CRITICAL: Unhandled error in Deferred:
2016-12-04 11:37:06 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "F:\Python35\lib\site-packages\twisted\internet\defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "F:\Python35\lib\site-packages\scrapy\crawler.py", line 73, in crawl
    start_requests = iter(self.spider.start_requests())
TypeError: 'NoneType' object is not iterable
